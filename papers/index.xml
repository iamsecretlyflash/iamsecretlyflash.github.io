<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Publications on Vaibhav Seth</title>
    <link>https://iamsecretlyflash.github.io/papers/</link>
    <description>Recent content in Publications on Vaibhav Seth</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://iamsecretlyflash.github.io/papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation</title>
      <link>https://iamsecretlyflash.github.io/papers/paper1/</link>
      <pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://iamsecretlyflash.github.io/papers/paper1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vaibhav Seth, Ayan Sengupta, Arinjay Pathak, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
&lt;strong&gt;Abstract:&lt;/strong&gt; Fine-tuning Large Language Models (LLMs) is resource-intensive due to their size. While low-rank adaptation is a common parameter-efficient fine-tuning approach, it is sensitive to hyperparameter choices, leading to instability. This paper introduces MonteCLoRA, a fine-tuning method that uses Monte Carlo estimation to achieve unbiased posterior estimation of low-rank parameters with low variance. MonteCLoRA improves stability and performance with minimal additional parameters (O(1)).
&lt;strong&gt;Under Review:&lt;/strong&gt; Journal of Machine Learning Research (JMLR), 2025
&lt;a href=&#34;https://arxiv.org/html/2411.04358v1&#34;&gt;Read the paper here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
