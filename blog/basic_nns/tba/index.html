<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What exactly is universal approximation about? | Vaibhav Seth</title>
<meta name="keywords" content="">
<meta name="description" content="If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that
makes these machines so powerful.
Universal Approximation Theorem

The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
">
<meta name="author" content="">
<link rel="canonical" href="https://iamsecretlyflash.github.io/blog/basic_nns/tba/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://iamsecretlyflash.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://iamsecretlyflash.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://iamsecretlyflash.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://iamsecretlyflash.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://iamsecretlyflash.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://iamsecretlyflash.github.io/blog/basic_nns/tba/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript> <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"
  integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"
  integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);"
></script>
 
<meta property="og:url" content="https://iamsecretlyflash.github.io/blog/basic_nns/tba/">
  <meta property="og:site_name" content="Vaibhav Seth">
  <meta property="og:title" content="What exactly is universal approximation about?">
  <meta property="og:description" content="If you have read of neural networks, there’s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.
Universal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous. ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-01-19T18:13:33+05:30">
    <meta property="article:modified_time" content="2025-01-19T18:13:33+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What exactly is universal approximation about?">
<meta name="twitter:description" content="If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that
makes these machines so powerful.
Universal Approximation Theorem

The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://iamsecretlyflash.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What exactly is universal approximation about?",
      "item": "https://iamsecretlyflash.github.io/blog/basic_nns/tba/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What exactly is universal approximation about?",
  "name": "What exactly is universal approximation about?",
  "description": "If you have read of neural networks, there\u0026rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.\nUniversal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\), provided the activation function is non-constant, bounded, and continuous. ",
  "keywords": [
    
  ],
  "articleBody": "If you have read of neural networks, there’s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.\nUniversal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\), provided the activation function is non-constant, bounded, and continuous. Formally: Let \\(K \\subset \\mathbb{R}^n\\) be a compact set and \\( f: K \\to \\mathbb{R} \\) be a continuous function. For any \\( \\epsilon \u003e 0 \\), there exists a feedforward neural network \\( g(x) \\) of the form: $$ g(x) = \\sum_{i=1}^m c_i * a(w_i^\\top x + b_i), $$ where:\n- \\( a: \\mathbb{R} \\to \\mathbb{R} \\) is the activation function, - \\( m \\) is the number of neurons in the hidden layer, - \\( c_i \\in \\mathbb{R} \\), \\( w_i \\in \\mathbb{R}^n \\), and \\( b_i \\in \\mathbb{R} \\) are parameters of the network, such that: $$ \\sup_{x \\in K} |f(x) - g(x)| \u003c \\epsilon. $$\nThis demonstrates that the network \\( g(x) \\) can approximate \\( f(x) \\) to arbitrary precision, given sufficient neurons in the hidden layer. Uhhhhh WHAT? To understand what the above theorem means, we will consider a simple neural network with a simple activation and assume that we are trying to approximate a one dimensional function.\nWe will get some definitions sorted first.\nLet's call the function to be approximated as \\( f(x) : \\mathbb{R} \\to \\mathbb{R} \\) and the approximating function as \\( \\hat{f}(x) : \\mathbb{R} \\to \\mathbb{R} \\) Activation Function Basically any non-linear function that transformrs the input. Some most common ones are with there graphs are:\n- Rectified Linear Unit (ReLU) : \\( max(x, 0) \\) - Sigmoid : \\( \\frac{1}{1 + \\exp(-x)} \\) ",
  "wordCount" : "307",
  "inLanguage": "en",
  "datePublished": "2025-01-19T18:13:33+05:30",
  "dateModified": "2025-01-19T18:13:33+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://iamsecretlyflash.github.io/blog/basic_nns/tba/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Vaibhav Seth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://iamsecretlyflash.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://iamsecretlyflash.github.io/" accesskey="h" title="Vaibhav Seth (Alt + H)">Vaibhav Seth</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://iamsecretlyflash.github.io/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://iamsecretlyflash.github.io/papers/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      What exactly is universal approximation about?
    </h1>
    <div class="post-meta"><span title='2025-01-19 18:13:33 +0530 IST'>2025-01-19 18:13</span>

</div>
  </header> 
  <div class="post-content"><p>If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that
makes these machines so powerful.</p>
<h3 id="universal-approximation-theorem">Universal Approximation Theorem<a hidden class="anchor" aria-hidden="true" href="#universal-approximation-theorem">#</a></h3>
<p>
The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
</p>
Formally:
<p>
Let \(K \subset \mathbb{R}^n\) be a compact set and \( f: K \to \mathbb{R} \) be a continuous function. For any \( \epsilon > 0 \), there exists a feedforward neural network \( g(x) \) of the form:
</p>
$$
g(x) = \sum_{i=1}^m c_i * a(w_i^\top x + b_i),
$$
<p>where:</p>
<p>
- \( a: \mathbb{R} \to \mathbb{R} \) is the activation function,
</p>
<p>
- \( m \) is the number of neurons in the hidden layer,
</p>
<p>
- \( c_i \in \mathbb{R} \), \( w_i \in \mathbb{R}^n \), and \( b_i \in \mathbb{R} \) are parameters of the network,
</p>
such that:
<p>$$
\sup_{x \in K} |f(x) - g(x)| &lt; \epsilon.
$$</p>
<p>
This demonstrates that the network \( g(x) \) can approximate \( f(x) \) to arbitrary precision, given sufficient neurons in the hidden layer.
</p>
<h2 id="uhhhhh-what">Uhhhhh WHAT?<a hidden class="anchor" aria-hidden="true" href="#uhhhhh-what">#</a></h2>
<p>To understand what the above theorem means, we will consider a simple neural network with a simple activation and assume that we are trying to approximate a one dimensional function.</p>
<p>We will get some definitions sorted first.</p>
<p>
Let's call the function to be approximated as \( f(x) : \mathbb{R} \to \mathbb{R} \) and the approximating function as \( \hat{f}(x) : \mathbb{R} \to \mathbb{R} \)
</p>
<h3 id="activation-function">Activation Function<a hidden class="anchor" aria-hidden="true" href="#activation-function">#</a></h3>
<p>Basically any non-linear function that transformrs the input. Some most common ones are with there graphs are:</p>
<p>
- Rectified Linear Unit (ReLU) : \( max(x, 0) \)
</p>
<p><img alt="alt text" loading="lazy" src="/images/relu.png"></p>
<p>
- Sigmoid : \( \frac{1}{1 + \exp(-x)} \)
</p>
<p><img alt="alt text" loading="lazy" src="/images/sigmoid.png"></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://iamsecretlyflash.github.io/">Vaibhav Seth</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
