<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What exactly is universal approximation about? | Vaibhav Seth</title>
<meta name="keywords" content="">
<meta name="description" content="If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.
Universal Approximation Theorem

The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
">
<meta name="author" content="">
<link rel="canonical" href="https://iamsecretlyflash.github.io/blog/universalapprox/article/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://iamsecretlyflash.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://iamsecretlyflash.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://iamsecretlyflash.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://iamsecretlyflash.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://iamsecretlyflash.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://iamsecretlyflash.github.io/blog/universalapprox/article/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript> <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"
  integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"
  integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);"
></script>
 
<meta property="og:url" content="https://iamsecretlyflash.github.io/blog/universalapprox/article/">
  <meta property="og:site_name" content="Vaibhav Seth">
  <meta property="og:title" content="What exactly is universal approximation about?">
  <meta property="og:description" content="If you have read of neural networks, there’s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.
Universal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous. ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-01-19T18:13:33+05:30">
    <meta property="article:modified_time" content="2025-01-19T18:13:33+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What exactly is universal approximation about?">
<meta name="twitter:description" content="If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.
Universal Approximation Theorem

The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://iamsecretlyflash.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What exactly is universal approximation about?",
      "item": "https://iamsecretlyflash.github.io/blog/universalapprox/article/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What exactly is universal approximation about?",
  "name": "What exactly is universal approximation about?",
  "description": "If you have read of neural networks, there\u0026rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.\nUniversal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\), provided the activation function is non-constant, bounded, and continuous. ",
  "keywords": [
    
  ],
  "articleBody": "If you have read of neural networks, there’s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.\nUniversal Approximation Theorem The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\), provided the activation function is non-constant, bounded, and continuous. Formally: Let \\(K \\subset \\mathbb{R}\\) be a compact set and \\( f: K \\to \\mathbb{R} \\) be a continuous function. For any \\( \\epsilon \u003e 0 \\), there exists a feedforward neural network \\( g(x) \\) of the form: $$ g(x) = \\sum_{i=1}^m c_i * a(w_i^\\top x + b_i), $$ where:\n- \\( a: \\mathbb{R} \\to \\mathbb{R} \\) is the activation function, - \\( m \\) is the number of neurons in the hidden layer, - \\( c_i \\in \\mathbb{R} \\), \\( w_i \\in \\mathbb{R} \\), and \\( b_i \\in \\mathbb{R} \\) are parameters of the network, such that: $$ \\sup_{x \\in K} |f(x) - g(x)| \u003c \\epsilon. $$\nThis demonstrates that the network \\( g(x) \\) can approximate \\( f(x) \\) to arbitrary precision, given sufficient neurons in the hidden layer. Uhhhhh WHAT? To understand what the above theorem means, we will consider a simple neural network with a simple activation and assume that we are trying to approximate a one-dimensional function.\nBut before anything, we will get some definitions sorted first. 1. Approximation function Let's call the function to be approximated as \\( f(x) : \\mathbb{R} \\to \\mathbb{R} \\) and the approximating function as \\( \\hat{f}(x) : \\mathbb{R} \\to \\mathbb{R} \\) 2. Activation Function Basically, any non-linear function that transforms the input. Some most common ones with their graphs are:\n- Rectified Linear Unit (ReLU) : \\( \\max(x, 0) \\) - Sigmoid : \\( \\frac{1}{1 + \\exp(-x)} \\) 3. Neural Network Basically a summation of functions over and over again.\nThe neural network described in the image works as follows:\nTakes a single input (x) and sends it to the three nodes. Each of those nodes first maps this input to a different space using a linear function of the form (a*x + b). At each node, a non-linear activation function is applied to the mapped input and passed to the next layer. In the above diagram, the next layer just takes a summation of the outputs of all the three nodes, and that is the output of the neural network. When relevant parameters are learned, that is the approximated function. A slightly more complicated neural network is described below:\nIn this network, the hidden units again perform the same steps as above, but this time the linear function has two input parameters. We can extend this format for any dimension of input.\nTo sort of complete things, here is another type of neural network:\nWhat you see above is a multiple-layer network. You can think of this recursively: the outputs from the first layer ((f_i)) become the inputs to the second layer. This recursive definition can be used to define any neural network of arbitrary depth.\nYou might have heard of multiple-layer neural networks performing exceptionally powerfully, but in theory, we can approximate any function using just a single hidden layer.\n4. Piecewise Linear Functions The final piece that we will require in this demo is the concept of a piecewise linear function. Let’s look at some graphs to understand them:\nThe above function is a basic ReLU function. As you can see, in the two regions where it’s differentiable, the function is linear. This is called piecewise linearity.\nThis one has three linear regions.\nThis one has four linear regions. All the graphs have been made by summing different ReLU functions. So you can see that adding ReLU basically gives a piecewise linear function.\nWith these basic definitions, let’s look at the universal approximation theorem.\nUniversal Approximation - A Visualization In this visualization of the theorem, we will be approximating a very basic quadratic function (just (x^2)) using a single hidden layer neural network, and the activation we will be using here will be ReLU. We will vary the number of hidden units and look at the results of the approximation. The learning of the weights will be performed using the backpropagation algorithm, and the visualization will be done using matplotlib.\nIf \\(N\\) is the number of hidden units, the form of the approximation by a single hidden layer network can be written as: $$ \\hat{f}(x) = \\sum_{i=1}^N c_i * ReLU(a_i * x + b_i) $$ Where \\(a_i, b_i\\), and \\(c_i\\) are constants that will be learned by the backpropagation algorithm by minimizing a certain loss function. We will not go into the backpropagation algorithm in this blog, so we will just assume that the weights returned by backprop are optimal.\nSumming ReLUs The first thing we will try here is to show how summing up ReLUs can potentially approximate a quadratic curve.\nIn the above figure, we have plotted \\(y = x^2\\) and \\(ReLU(x)\\). As you can observe, the approximation is pretty terrible here, and there are just two points where our approximation function hits the quadratic curve. However, this serves as a good starting point for us to understand how we can approximate functions using the universal approximation theorem. Next, we will add one more ReLU function, and we will do it a little smartly. We will add it in such a way that the resulting piecewise continuous function comes closer to the right side of the quadratic curve. In the above graph, we have added (ReLU(x - 1)). The idea behind this was that the slope at (x = 1) is 2, and we want to ensure that the curve aligns better.\nNow we do a symmetric addition of (ReLU(-x + 1)) to the function. This will ensure that the curve aligns better on the left side. As you can observe the curve is now much closer to the quadratic curve. We can keep adding ReLU functions to get closer to the curve. The more ReLU functions we add, the closer we get to the curve.\nVarying the number of hidden units - Animation We will now look at how the approximation changes as we vary the number of hidden units. We will be using the same quadratic function and ReLU activation function. We will also be looking at the area under the curve of the difference between the approximated function and the quadratic function in the domain [-2,2] to understand how the approximation is improving.\nAs you can observe in the above animation, the approximation gets better as we increase the number of hidden units. The area under the curve of the difference between the approximated function and the quadratic function decreases as we increase the number of hidden units.\nWith this demonstration, we can now move onto the proof of the universal approximation theorem. Proof of the Universal Approximation Theorem for 1D Case We are going to look at a special case of Universal Approximation using a Shallow Neural Network. We will be approximation univariate functions with univariate inputs.\nLet \\(sigma: \\mathbb{R} \\to \\mathbb{R}\\) be a continuous, non-constant, and sigmoidal activation function. A standard example of a sigmoidal function is one satisfying \\( lim_{x \\to -\\infty} \\) \\(sigma(x) = 0 \\) and \\( lim_{x \\to +\\infty} \\sigma(x) = 1.\\) Then for any continuous function \\(f: [a,b]\\to \\mathbb{R} \\) and any \\( \\varepsilon \u003e 0 \\), there exist \\(m \\in \\mathbb{N}\\) and real coefficients \\( v_0,,v_i,w_i ,b_i\\) \\( 1 \\le i \\le m\\) such that $$ \\max_{x \\in [a,b]} \\Bigl|f(x) -\\Bigl[v_0+\\sum_{i=1}^{m} v_i \\sigma\\bigl(w_ix + b_i\\bigr)\\Bigr]\\Bigr|\u003c\\varepsilon $$\nIn other words, single-hidden-layer neural networks with a sigmoidal activation can uniformly approximate any continuous function on a compact interval as closely as we want.\nWe build the proof in several phases:\n1. Show that \\( \\sigma(\\alpha(x - t)) \\) approximates a step function.\n2. Use step functions to approximate continuous functions by piecewise constants. Replace exact step functions in the piecewise-constant approximation with sigmoidal “smoothed” steps. Combine the two approximations to control the overall error. Phase 1 - Sigmoid Approximates Step Function Define the Heaviside (unit) step function \\( H : \\mathbb{R} \\to \\{0, 1\\} \\) by $$ H(u) = \\begin{cases} 0, \u0026 u \u003c 0, \\\\ 1, \u0026 u \\geq 0. \\end{cases} $$ A key property of a sigmoidal \\( \\sigma \\) is: $$ \\lim_{z \\to -\\infty} \\sigma(z) = 0, \\quad \\lim_{z \\to +\\infty} \\sigma(z) = 1. $$ Given any \\( \\delta \u003e 0 \\) and a bounded interval \\([A, B] \\subset \\mathbb{R}\\), there exists a sufficiently large scalar \\( \\alpha \u003e 0 \\) (depending on \\( \\delta \\) and \\([A, B]\\)) such that, for every \\( x \\in [A, B] \\), $$ \\lvert \\sigma(\\alpha x) - H(x) \\rvert \u003c \\delta. $$ Intuitively, as \\( \\alpha \\to \\infty \\), the graph of \\( y = \\sigma(\\alpha x) \\) becomes a steeper and steeper \"S-curve\" approximating the jump at \\( 0 \\). If we want a \"jump\" around \\( x = t \\), we look at \\( \\sigma(\\alpha(x - t)) \\). For large \\( \\alpha \\), this approximates $$ x \\mapsto H(x - t) = \\begin{cases} 0, \u0026 x \u003c t, \\\\ 1, \u0026 x \\geq t. \\end{cases} $$ We can make\n$$ \\max_{x \\in [a, b]} \\lvert \\sigma(\\alpha(x - t)) - H(x - t) \\rvert $$\narbitrarily small by choosing \\( \\alpha \\) large enough (the choice depends on how large \\([a, b]\\) is and on \\( \\delta \\)). Phase 2.1 - Piecewise Constant Approximation By the uniform continuity of \\( f \\) on the compact interval \\([a, b]\\), for any \\( \\epsilon \u003e 0 \\), there is a partition of \\([a, b]\\): \\( a = x_0 \u003c x_1 \u003c x_2 \u003c .... \u003c x_{N-1} \u003c x_N = b \\) so that on each subinterval \\([x_{k-1}, x_k]\\), the oscillation of \\( f \\) is at most \\( \\epsilon / 2 \\). Concretely, pick some point \\( \\xi_k \\in [x_{k-1}, x_k] \\) and let \\[ c_k = f(\\xi_k). \\] Then define a piecewise-constant approximation \\( F \\) by \\[ F(x) = \\sum_{k=1}^{N} c_k \\cdot \\mathbf{1}_{[x_{k-1}, x_k)}(x). \\] Equivalently,\n$$ F(x) = \\sum_{k=1}^{N} c_k \\left[ H(x - x_{k-1}) - H(x - x_k) \\right].$$\nEach bracket is 1 precisely when \\( x \\in [x_{k-1}, x_k) \\) and 0 otherwise. By construction, $$ \\max_{x \\in [a, b]} \\left| f(x) - F(x) \\right| \\leq \\frac{\\epsilon}{2}. $$ Phase 2.2 - How does Uniform Continuity help with the interval partition? A standard result in real analysis is:\nHeine–Cantor Theorem: If a function ( f ) is continuous on a compact set (here ([a, b])), then ( f ) is uniformly continuous.\nUniform continuity says:\nFor every \\(\\epsilon' \u003e 0\\), there exists a \\(\\delta \u003e 0\\) such that for all \\( x, y\\in [a, b]\\), if \\( |x - y| \u003c \\delta\\) , then \\(|f(x) - f(y)| \u003c \\epsilon'\\). We will choose \\( \\epsilon' := \\frac{\\epsilon}{2} \\). So: Given \\( \\epsilon \u003e 0 \\), pick \\( \\delta \u003e 0 \\) such that for any \\( x, y \\) with \\( |x - y| \u003c \\delta \\), we have $$ |f(x) - f(y)| \u003c \\frac{\\epsilon}{2}. $$ That’s the key.\nWith this \\( \\delta \u003e 0 \\) in hand, we subdivide \\([a, b]\\) into \\( N \\) intervals so small that the maximum length of each subinterval is strictly less than \\( \\delta \\). One standard way: 1. Choose \\( N \\) so that $$ \\frac{b - a}{N} \u003c \\delta. $$ Define the partition points: $$ x_i = a + i \\cdot \\frac{b - a}{N}, \\quad i = 0, 1, \\ldots, N. $$\nThen each subinterval \\([x_{i-1}, x_i]\\) has length $$ \\frac{b - a}{N}, $$ which is \\( \u003c \\delta \\). Concretely, $$ a = x_0 \u003c x_1 \u003c x_2 \u003c \\cdots \u003c x_{N-1} \u003c x_N = b, $$\nwith\n$$ x_i - x_{i-1} = \\frac{b - a}{N} \u003c \\delta. $$\nNow, for any \\( i = 1, \\ldots, N \\) and for any \\( x, y \\in [x_{i-1}, x_i] \\), we have $$ |x - y| \\leq x_i - x_{i-1} = \\frac{b - a}{N} \u003c \\delta. $$ Hence, by the choice of \\( \\delta \\), $$ |f(x) - f(y)| \u003c \\frac{\\epsilon}{2}. $$ In other words, the function's oscillation on each subinterval \\([x_{i-1}, x_i]\\) is at most \\( \\frac{\\epsilon}{2} \\). Formally, $$ \\sup_{x, y \\in [x_{i-1}, x_i]} |f(x) - f(y)| \\leq \\frac{\\epsilon}{2}. $$ That’s exactly what we wanted to show.\nPhase 3 - Sigmoid Again Now, we replace the ideal step \\( H(\\cdot) \\) by the approximate step \\( \\sigma(\\alpha(\\cdot)) \\). Define $$ F_\\alpha(x) = \\sum_{k=1}^N c_k \\left[ \\sigma\\left(\\alpha (x - x_{k-1})\\right) - \\sigma\\left(\\alpha (x - x_k)\\right) \\right] $$ Each term \\( \\sigma\\left(\\alpha (x - x_{k-1})\\right) \\) approximates \\( H(x - x_{k-1}) \\), and similarly for \\( \\sigma\\left(\\alpha (x - x_k)\\right) \\). As a result, for sufficiently large \\( \\alpha \\): $$ \\left| F(x) - F_\\alpha(x) \\right| \\le \\sum_{k=1}^N c_k \\left( \\left [ H(x - x_{k-1}) - \\sigma\\left(\\alpha (x - x_{k-1})\\right) \\right] - \\left[\\left( H(x - x_{k}) - \\sigma\\left(\\alpha (x - x_k)\\right) \\right) \\right] \\right) $$ $$ \\left| F(x) - F_\\alpha(x) \\right| \\le \\sum_{k=1}^N |c_k| \\left| \\left [ H(x - x_{k-1}) - \\sigma\\left(\\alpha (x - x_{k-1})\\right) \\right] | + |\\left( H(x - x_{k}) - \\sigma\\left(\\alpha (x - x_k)\\right) \\right) \\right)| $$\n$$ \\left| F(x) - F_\\alpha(x) \\right| \\le 2N * \\beta * |c_{max}| $$\nwhere \\( \\beta \\) can be arbitrarily selected and \\( c_{max} \\) is the maximum value of \\( c_k \\). We select beta such that the whole equation is less than \\( \\frac{\\epsilon}{2} \\). Phase 4 - Combine the Errors We already have\n$$ \\max_{x \\in [a, b]} \\lvert f(x) - F(x) \\rvert \\leq \\frac{\\epsilon}{2}. $$\nAlso, we have\n$$ \\max_{x \\in [a, b]} \\lvert F(x) - F_\\alpha(x) \\rvert \\leq \\frac{\\epsilon}{2}. $$\nBy the triangle inequality,\n$$ \\max_{x \\in [a, b]} \\lvert f(x) - F_\\alpha(x) \\rvert \\leq \\max_{x \\in [a, b]} \\lvert f(x) - F(x) \\rvert + \\max_{x \\in [a, b]} \\lvert F(x) - F_\\alpha(x) \\rvert \u003c \\epsilon. $$\nThus, \\( F_\\alpha \\) is within \\( \\epsilon \\) of \\( f \\) uniformly on \\([a, b]\\). Phase 5 - Expressing the approximation as a Single-Hidden-Layer Network Recall:\n$$ F_\\alpha(x) = \\sum_{k=1}^N c_k \\left[ \\sigma\\left(\\alpha (x - x_{k-1})\\right) - \\sigma\\left(\\alpha (x - x_k)\\right) \\right]. $$\nNotice that each bracketed term is a linear combination of sigmoids \\( \\sigma(\\alpha x + b) \\) for some bias \\( b \\) (specifically, \\( b = -\\alpha x_{k-1} \\) or \\( b = -\\alpha x_k \\)). If we label each instance of \\( \\sigma(\\cdot) \\) as a \"hidden neuron,\" then \\( F_\\alpha \\) is exactly $$ v_0 + \\sum_{i=1}^{2N} v_i \\sigma(w_i x + b_i), $$ where:\n- \\( 2N \\) is the total count of sigmoids (two for each subinterval, though some proofs can combine them cleverly), - \\( w_i = \\pm \\alpha \\), and - \\( v_i, b_i \\) are real coefficients constructed from the \\(\\{c_k\\}\\) and the partition \\(\\{x_k\\}\\). - \\( v_0 \\) may be \\( 0 \\) or another simple constant offset if needed. Hence, the function \\( F_\\alpha \\) is literally a single-hidden-layer neural network with a sigmoidal activation. Putting it all together, we see that for any \\( \\epsilon \u003e 0 \\), we can find such an \\( F_\\alpha \\) so that $$ \\max_{x \\in [a, b]} \\lvert f(x) - F_\\alpha(x) \\rvert \u003c \\epsilon. $$ This completes the proof that single-hidden-layer networks (with \\( \\sigma \\) sigmoidal) can approximate continuous functions on a compact interval arbitrarily well. In the above five phases of the proof, we have shown that a single-hidden-layer neural network with a sigmoidal activation function can approximate any continuous function on a compact interval as closely as we want. This is the essence of the Universal Approximation Theorem.\nConclusion The Universal Approximation Theorem is a fundamental result in the theory of neural networks. It states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\), provided the activation function is non-constant, bounded, and continuous. The theorem is a key principle that makes neural networks so powerful. It demonstrates that the network can approximate any function to arbitrary precision, given sufficient neurons in the hidden layer. This is the reason why neural networks are so widely used in machine learning and artificial intelligence.\nThere are proofs that generalize the proof the multi-dimensional case, but the essence remains the same. The theorem is a powerful tool that underpins the success of neural networks in a wide range of applications.\nProofs for other activation functions and deeper networks are also available, but the single-hidden-layer case is the most fundamental and widely used. You can refer to the original paper by Kurt Hornik for more details.\nI hope this blog helped you understand the Universal Approximation Theorem better. If you have any questions or feedback, feel free to reach out to me. Thanks for reading!\n",
  "wordCount" : "2833",
  "inLanguage": "en",
  "datePublished": "2025-01-19T18:13:33+05:30",
  "dateModified": "2025-01-19T18:13:33+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://iamsecretlyflash.github.io/blog/universalapprox/article/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Vaibhav Seth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://iamsecretlyflash.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://iamsecretlyflash.github.io/" accesskey="h" title="Vaibhav Seth (Alt + H)">Vaibhav Seth</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://iamsecretlyflash.github.io/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://iamsecretlyflash.github.io/papers/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      What exactly is universal approximation about?
    </h1>
    <div class="post-meta"><span title='2025-01-19 18:13:33 +0530 IST'>2025-01-19 18:13</span>

</div>
  </header> 
  <div class="post-content"><p>If you have read of neural networks, there&rsquo;s some chance you must have heard of the universal approximation theorem. The key principle that makes these machines so powerful.</p>
<h3 id="universal-approximation-theorem">Universal Approximation Theorem<a hidden class="anchor" aria-hidden="true" href="#universal-approximation-theorem">#</a></h3>
<p>
The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
</p>
Formally:
<p>
Let \(K \subset \mathbb{R}\) be a compact set and \( f: K \to \mathbb{R} \) be a continuous function. For any \( \epsilon > 0 \), there exists a feedforward neural network \( g(x) \) of the form:
</p>
$$
g(x) = \sum_{i=1}^m c_i * a(w_i^\top x + b_i),
$$
<p>where:</p>
<p>
- \( a: \mathbb{R} \to \mathbb{R} \) is the activation function,
</p>
<p>
- \( m \) is the number of neurons in the hidden layer,
</p>
<p>
- \( c_i \in \mathbb{R} \), \( w_i \in \mathbb{R} \), and \( b_i \in \mathbb{R} \) are parameters of the network,
</p>
such that:
<p>$$
\sup_{x \in K} |f(x) - g(x)| &lt; \epsilon.
$$</p>
<p>
This demonstrates that the network \( g(x) \) can approximate \( f(x) \) to arbitrary precision, given sufficient neurons in the hidden layer.
</p>
<h2 id="uhhhhh-what">Uhhhhh WHAT?<a hidden class="anchor" aria-hidden="true" href="#uhhhhh-what">#</a></h2>
<p>To understand what the above theorem means, we will consider a simple neural network with a simple activation and assume that we are trying to approximate a one-dimensional function.</p>
<h3 id="but-before-anything-we-will-get-some-definitions-sorted-first">But before anything, we will get some definitions sorted first.<a hidden class="anchor" aria-hidden="true" href="#but-before-anything-we-will-get-some-definitions-sorted-first">#</a></h3>
<h3 id="1-approximation-function">1. Approximation function<a hidden class="anchor" aria-hidden="true" href="#1-approximation-function">#</a></h3>
<p>
Let's call the function to be approximated as \( f(x) : \mathbb{R} \to \mathbb{R} \) and the approximating function as \( \hat{f}(x) : \mathbb{R} \to \mathbb{R} \)
</p>
<h3 id="2-activation-function">2. Activation Function<a hidden class="anchor" aria-hidden="true" href="#2-activation-function">#</a></h3>
<p>Basically, any non-linear function that transforms the input. Some most common ones with their graphs are:</p>
<p>
- Rectified Linear Unit (ReLU) : \( \max(x, 0) \)
</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/relu.png"></p>
<p>
- Sigmoid : \( \frac{1}{1 + \exp(-x)} \)
</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/sigmoid.png"></p>
<h3 id="3-neural-network">3. Neural Network<a hidden class="anchor" aria-hidden="true" href="#3-neural-network">#</a></h3>
<p>Basically a summation of functions over and over again.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/basic_nn.png"></p>
<p>The neural network described in the image works as follows:</p>
<ul>
<li>Takes a single input (x) and sends it to the three nodes.</li>
<li>Each of those nodes first maps this input to a different space using a linear function of the form (a*x + b).</li>
<li>At each node, a non-linear activation function is applied to the mapped input and passed to the next layer.</li>
<li>In the above diagram, the next layer just takes a summation of the outputs of all the three nodes, and that is the output of the neural network. When relevant parameters are learned, that is the approximated function.</li>
</ul>
<p>A slightly more complicated neural network is described below:</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/multivariate_nn.png"></p>
<p>In this network, the hidden units again perform the same steps as above, but this time the linear function has two input parameters. We can extend this format for any dimension of input.</p>
<p>To sort of complete things, here is another type of neural network:</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/multi_layer_nn.png"></p>
<p>What you see above is a multiple-layer network. You can think of this recursively: the outputs from the first layer ((f_i)) become the inputs to the second layer. This recursive definition can be used to define any neural network of arbitrary depth.</p>
<p>You might have heard of multiple-layer neural networks performing exceptionally powerfully, but in theory, we can approximate any function using just a single hidden layer.</p>
<h3 id="4-piecewise-linear-functions">4. Piecewise Linear Functions<a hidden class="anchor" aria-hidden="true" href="#4-piecewise-linear-functions">#</a></h3>
<p>The final piece that we will require in this demo is the concept of a piecewise linear function. Let&rsquo;s look at some graphs to understand them:</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/piecewise_1.png"></p>
<p>The above function is a basic ReLU function. As you can see, in the two regions where it&rsquo;s differentiable, the function is linear. This is called piecewise linearity.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/piecewise_2.png"></p>
<p>This one has three linear regions.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/piecewise_3.png"></p>
<p>This one has four linear regions. All the graphs have been made by summing different ReLU functions. So you can see that adding ReLU basically gives a piecewise linear function.</p>
<p>With these basic definitions, let&rsquo;s look at the universal approximation theorem.</p>
<h2 id="universal-approximation---a-visualization">Universal Approximation - A Visualization<a hidden class="anchor" aria-hidden="true" href="#universal-approximation---a-visualization">#</a></h2>
<p>In this visualization of the theorem, we will be approximating a very basic quadratic function (just (x^2)) using a single hidden layer neural network, and the activation we will be using here will be ReLU. We will vary the number of hidden units and look at the results of the approximation. The learning of the weights will be performed using the backpropagation algorithm, and the visualization will be done using matplotlib.</p>
<p>
If \(N\) is the number of hidden units, the form of the approximation by a single hidden layer network can be written as:
</p>
$$
\hat{f}(x) = \sum_{i=1}^N c_i * ReLU(a_i * x + b_i)
$$
<p>
Where \(a_i, b_i\), and \(c_i\) are constants that will be learned by the backpropagation algorithm by minimizing a certain loss function.
</p>
<p>We will not go into the backpropagation algorithm in this blog, so we will just assume that the weights returned by backprop are optimal.</p>
<h3 id="summing-relus">Summing ReLUs<a hidden class="anchor" aria-hidden="true" href="#summing-relus">#</a></h3>
<p>The first thing we will try here is to show how summing up ReLUs can potentially approximate a quadratic curve.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/approx_quadratic_1.png"></p>
<p>
In the above figure, we have plotted \(y = x^2\) and \(ReLU(x)\). As you can observe, the approximation is pretty terrible here, and there are just two points where our approximation function hits the quadratic curve. However, this serves as a good starting point for us to understand how we can approximate functions using the universal approximation theorem.
</p>
Next, we will add one more ReLU function, and we will do it a little smartly. We will add it in such a way that the resulting piecewise continuous function comes closer to the right side of the quadratic curve.
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/approx_quadratic_2.png"></p>
<p>In the above graph, we have added (ReLU(x - 1)). The idea behind this was that the slope at (x = 1) is 2, and we want to ensure that the curve aligns better.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/approx_quadratic_3.png"></p>
<p>Now we do a symmetric addition of (ReLU(-x + 1)) to the function. This will ensure that the curve aligns better on the left side.
<img alt="alt text" loading="lazy" src="/images/blog/universalapprox/approx_quadratic_4.png"></p>
<p>As you can observe the curve is now much closer to the quadratic curve. We can keep adding ReLU functions to get closer to the curve. The more ReLU functions we add, the closer we get to the curve.</p>
<h3 id="varying-the-number-of-hidden-units---animation">Varying the number of hidden units - Animation<a hidden class="anchor" aria-hidden="true" href="#varying-the-number-of-hidden-units---animation">#</a></h3>
<p>We will now look at how the approximation changes as we vary the number of hidden units. We will be using the same quadratic function and ReLU activation function. We will also be looking at the area
under the curve of the difference between the approximated function and the quadratic function in the domain [-2,2] to understand how the approximation is improving.</p>
<p><img alt="alt text" loading="lazy" src="/images/blog/universalapprox/nn_animation.gif"></p>
<p>As you can observe in the above animation, the approximation gets better as we increase the number of hidden units. The area under the curve of the difference between the approximated function and the quadratic function decreases as we increase the number of hidden units.</p>
<h3 id="with-this-demonstration-we-can-now-move-onto-the-proof-of-the-universal-approximation-theorem">With this demonstration, we can now move onto the proof of the universal approximation theorem.<a hidden class="anchor" aria-hidden="true" href="#with-this-demonstration-we-can-now-move-onto-the-proof-of-the-universal-approximation-theorem">#</a></h3>
<h2 id="proof-of-the-universal-approximation-theorem-for-1d-case">Proof of the Universal Approximation Theorem for 1D Case<a hidden class="anchor" aria-hidden="true" href="#proof-of-the-universal-approximation-theorem-for-1d-case">#</a></h2>
<p>We are going to look at a special case of Universal Approximation using a Shallow Neural Network. We will be approximation univariate functions with univariate inputs.</p>
<p>
Let \(sigma: \mathbb{R} \to \mathbb{R}\) be a continuous, non-constant, and sigmoidal activation function.
</p>
<p>
A standard example of a sigmoidal function is one satisfying \( lim_{x \to -\infty} \) \(sigma(x) = 0 \) and \( lim_{x \to +\infty} \sigma(x) = 1.\)
</p>
<p>
Then for any continuous function \(f: [a,b]\to \mathbb{R} \) and any \( \varepsilon > 0 \),
there exist  \(m \in \mathbb{N}\) and real coefficients  \( v_0,,v_i,w_i ,b_i\) \( 1 \le i \le m\) such that
</p>
<p>$$
\max_{x \in [a,b]}
\Bigl|f(x) -\Bigl[v_0+\sum_{i=1}^{m} v_i \sigma\bigl(w_ix + b_i\bigr)\Bigr]\Bigr|&lt;\varepsilon
$$</p>
<p>In other words, single-hidden-layer neural networks with a sigmoidal activation
can uniformly approximate any continuous function on a compact interval
as closely as we want.</p>
<p>We build the proof in several phases:</p>
<p>1. Show that \( \sigma(\alpha(x - t)) \) approximates a step function.</p>
2. Use step functions to approximate continuous functions by piecewise constants.
<ol start="3">
<li>Replace exact step functions in the piecewise-constant approximation with sigmoidal &ldquo;smoothed&rdquo; steps.</li>
<li>Combine the two approximations to control the overall error.</li>
</ol>
<h3 id="phase-1---sigmoid-approximates-step-function">Phase 1 - Sigmoid Approximates Step Function<a hidden class="anchor" aria-hidden="true" href="#phase-1---sigmoid-approximates-step-function">#</a></h3>
<p>
Define the Heaviside (unit) step function \( H : \mathbb{R} \to \{0, 1\} \) by
</p>
$$
H(u) =
\begin{cases}
0, & u < 0, \\
1, & u \geq 0.
\end{cases}
$$
<p>
A key property of a sigmoidal \( \sigma \) is:
</p>
$$
\lim_{z \to -\infty} \sigma(z) = 0, \quad \lim_{z \to +\infty} \sigma(z) = 1.
$$
<p>
Given any \( \delta > 0 \) and a bounded interval \([A, B] \subset \mathbb{R}\), there exists a sufficiently large scalar \( \alpha > 0 \) (depending on \( \delta \) and \([A, B]\)) such that, for every \( x \in [A, B] \),
</p>
$$
\lvert \sigma(\alpha x) - H(x) \rvert < \delta.
$$
<p>
Intuitively, as \( \alpha \to \infty \), the graph of \( y = \sigma(\alpha x) \) becomes a steeper and steeper "S-curve" approximating the jump at \( 0 \).
</p>
<p>
If we want a "jump" around \( x = t \), we look at \( \sigma(\alpha(x - t)) \). For large \( \alpha \), this approximates
</p>
$$
x \mapsto H(x - t) =
\begin{cases}
0, & x < t, \\
1, & x \geq t.
\end{cases}
$$
<p>We can make</p>
<p>$$
\max_{x \in [a, b]} \lvert \sigma(\alpha(x - t)) - H(x - t) \rvert
$$</p>
<p>
arbitrarily small by choosing \( \alpha \) large enough (the choice depends on how large \([a, b]\) is and on \( \delta \)).
</p>
<h3 id="phase-21---piecewise-constant-approximation">Phase 2.1 - Piecewise Constant Approximation<a hidden class="anchor" aria-hidden="true" href="#phase-21---piecewise-constant-approximation">#</a></h3>
<p>
By the uniform continuity of \( f \) on the compact interval \([a, b]\), for any \( \epsilon > 0 \), there is a partition of \([a, b]\):
</p>
<p>
\( a = x_0 < x_1 < x_2 < .... < x_{N-1} < x_N = b \)
</p>
<p>
so that on each subinterval \([x_{k-1}, x_k]\), the oscillation of \( f \) is at most \( \epsilon / 2 \). Concretely, pick some point \( \xi_k \in [x_{k-1}, x_k] \) and let
</p>
\[
c_k = f(\xi_k).
\]
<p>
Then define a piecewise-constant approximation \( F \) by
</p>
\[
F(x) = \sum_{k=1}^{N} c_k \cdot \mathbf{1}_{[x_{k-1}, x_k)}(x).
\]
<p>Equivalently,</p>
<p>$$
F(x) = \sum_{k=1}^{N} c_k \left[ H(x - x_{k-1}) - H(x - x_k) \right].$$</p>
<p>
Each bracket is 1 precisely when \( x \in [x_{k-1}, x_k) \) and 0 otherwise. By construction,
</p>
$$
\max_{x \in [a, b]} \left| f(x) - F(x) \right| \leq \frac{\epsilon}{2}.
$$
<h3 id="phase-22---how-does-uniform-continuity-help-with-the-interval-partition">Phase 2.2 - How does Uniform Continuity help with the interval partition?<a hidden class="anchor" aria-hidden="true" href="#phase-22---how-does-uniform-continuity-help-with-the-interval-partition">#</a></h3>
<p>A standard result in real analysis is:</p>
<p><strong>Heine–Cantor Theorem:</strong>
If a function ( f ) is continuous on a compact set (here ([a, b])), then ( f ) is <strong>uniformly continuous</strong>.</p>
<p>Uniform continuity says:</p>
<p>
For every \(\epsilon' > 0\), there exists a  \(\delta > 0\) such that for all  \( x, y\in [a, b]\), if \( |x - y| < \delta\) , then \(|f(x) - f(y)| < \epsilon'\).
</p>
<p>
We will choose \( \epsilon' := \frac{\epsilon}{2} \). So: Given \( \epsilon > 0 \), pick \( \delta > 0 \) such that for any \( x, y \) with \( |x - y| < \delta \), we have
</p>
$$
|f(x) - f(y)| < \frac{\epsilon}{2}.
$$
<p>That&rsquo;s the key.</p>
<p>
With this \( \delta > 0 \) in hand, we subdivide \([a, b]\) into \( N \) intervals so small that the maximum length of each subinterval is strictly less than \( \delta \). One standard way:
</p>
<p>
1. Choose \( N \) so that
</p>
$$
\frac{b - a}{N} < \delta.
$$
<ol start="2">
<li>Define the partition points:</li>
</ol>
<p>$$
x_i = a + i \cdot \frac{b - a}{N}, \quad i = 0, 1, \ldots, N.
$$</p>
<p>
Then each subinterval \([x_{i-1}, x_i]\) has length
</p>
$$
\frac{b - a}{N},
$$
<p>
which is \( < \delta \).
</p>
Concretely,
<p>$$
a = x_0 &lt; x_1 &lt; x_2 &lt; \cdots &lt; x_{N-1} &lt; x_N = b,
$$</p>
<p>with</p>
<p>$$
x_i - x_{i-1} = \frac{b - a}{N} &lt; \delta.
$$</p>
<p>
Now, for any \( i = 1, \ldots, N \) and for any \( x, y \in [x_{i-1}, x_i] \), we have
</p>
$$
|x - y| \leq x_i - x_{i-1} = \frac{b - a}{N} < \delta.
$$
<p>
Hence, by the choice of \( \delta \),
</p>
$$
|f(x) - f(y)| < \frac{\epsilon}{2}.
$$
<p>
In other words, the function's oscillation on each subinterval \([x_{i-1}, x_i]\) is at most \( \frac{\epsilon}{2} \). Formally,
</p>
$$
\sup_{x, y \in [x_{i-1}, x_i]} |f(x) - f(y)| \leq \frac{\epsilon}{2}.
$$
<p>That&rsquo;s exactly what we wanted to show.</p>
<h3 id="phase-3---sigmoid-again">Phase 3 - Sigmoid Again<a hidden class="anchor" aria-hidden="true" href="#phase-3---sigmoid-again">#</a></h3>
<p>
Now, we replace the ideal step \( H(\cdot) \) by the approximate step \( \sigma(\alpha(\cdot)) \). Define
</p>
$$
F_\alpha(x) = \sum_{k=1}^N c_k \left[ \sigma\left(\alpha (x - x_{k-1})\right) - \sigma\left(\alpha (x - x_k)\right) \right]
$$
<p>
Each term \( \sigma\left(\alpha (x - x_{k-1})\right) \) approximates \( H(x - x_{k-1}) \), and similarly for \( \sigma\left(\alpha (x - x_k)\right) \). As a result, for sufficiently large \( \alpha \):
</p>
$$
\left| F(x) - F_\alpha(x) \right| \le \sum_{k=1}^N c_k  \left( \left [ H(x - x_{k-1}) - \sigma\left(\alpha (x - x_{k-1})\right) \right] - \left[\left( H(x - x_{k}) - \sigma\left(\alpha (x - x_k)\right) \right) \right] \right)
$$
<p>$$
\left| F(x) - F_\alpha(x) \right| \le \sum_{k=1}^N |c_k|  \left| \left [ H(x - x_{k-1}) - \sigma\left(\alpha (x - x_{k-1})\right) \right] | + |\left( H(x - x_{k}) - \sigma\left(\alpha (x - x_k)\right) \right)  \right)|
$$</p>
<p>$$
\left| F(x) - F_\alpha(x) \right| \le 2N * \beta * |c_{max}|
$$</p>
<p>
where \( \beta \) can be arbitrarily selected and \( c_{max} \) is the maximum value of \( c_k \). We select beta such that the whole equation is less than \( \frac{\epsilon}{2} \).
</p>
<h3 id="phase-4---combine-the-errors">Phase 4 - Combine the Errors<a hidden class="anchor" aria-hidden="true" href="#phase-4---combine-the-errors">#</a></h3>
<p>We already have</p>
<p>$$
\max_{x \in [a, b]} \lvert f(x) - F(x) \rvert \leq \frac{\epsilon}{2}.
$$</p>
<p>Also, we have</p>
<p>$$
\max_{x \in [a, b]} \lvert F(x) - F_\alpha(x) \rvert \leq \frac{\epsilon}{2}.
$$</p>
<p>By the triangle inequality,</p>
<p>$$
\max_{x \in [a, b]} \lvert f(x) - F_\alpha(x) \rvert
\leq \max_{x \in [a, b]} \lvert f(x) - F(x) \rvert + \max_{x \in [a, b]} \lvert F(x) - F_\alpha(x) \rvert &lt; \epsilon.
$$</p>
<p>
Thus, \( F_\alpha \) is within \( \epsilon \) of \( f \) uniformly on \([a, b]\).
</p>
<h3 id="phase-5---expressing-the-approximation-as-a-single-hidden-layer-network">Phase 5 - Expressing the approximation as a Single-Hidden-Layer Network<a hidden class="anchor" aria-hidden="true" href="#phase-5---expressing-the-approximation-as-a-single-hidden-layer-network">#</a></h3>
<p>Recall:</p>
<p>$$
F_\alpha(x) = \sum_{k=1}^N c_k \left[ \sigma\left(\alpha (x - x_{k-1})\right) - \sigma\left(\alpha (x - x_k)\right) \right].
$$</p>
<p>
Notice that each bracketed term is a linear combination of sigmoids \( \sigma(\alpha x + b) \) for some bias \( b \) (specifically, \( b = -\alpha x_{k-1} \) or \( b = -\alpha x_k \)). If we label each instance of \( \sigma(\cdot) \) as a "hidden neuron," then \( F_\alpha \) is exactly
</p>
$$
v_0 + \sum_{i=1}^{2N} v_i \sigma(w_i x + b_i),
$$
<p>where:</p>
<p>
- \( 2N \) is the total count of sigmoids (two for each subinterval, though some proofs can combine them cleverly),
</p>
<p>
- \( w_i = \pm \alpha \), and
</p>
<p>
- \( v_i, b_i \) are real coefficients constructed from the \(\{c_k\}\) and the partition \(\{x_k\}\).
</p>
<p>
- \( v_0 \) may be \( 0 \) or another simple constant offset if needed.
</p>
<p>
Hence, the function \( F_\alpha \) is literally a single-hidden-layer neural network with a sigmoidal activation.
</p>
<p>
Putting it all together, we see that for any \( \epsilon > 0 \), we can find such an \( F_\alpha \) so that
</p>
$$
\max_{x \in [a, b]} \lvert f(x) - F_\alpha(x) \rvert < \epsilon.
$$
<p>
This completes the proof that single-hidden-layer networks (with \( \sigma \) sigmoidal) can approximate continuous functions on a compact interval arbitrarily well.
</p>
<p>In the above five phases of the proof, we have shown that a single-hidden-layer neural network with a sigmoidal activation function can approximate any continuous function on a compact interval as closely as we want. This is the essence of the Universal Approximation Theorem.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>
The Universal Approximation Theorem is a fundamental result in the theory of neural networks. It states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of \(\mathbb{R}^n\), provided the activation function is non-constant, bounded, and continuous.
</p>
<p>The theorem is a key principle that makes neural networks so powerful. It demonstrates that the network can approximate any function to arbitrary precision, given sufficient neurons in the hidden layer. This is the reason why neural networks are so widely used in machine learning and artificial intelligence.</p>
<p>There are proofs that generalize the proof the multi-dimensional case, but the essence remains the same. The theorem is a powerful tool that underpins the success of neural networks in a wide range of applications.</p>
<p>Proofs for other activation functions and deeper networks are also available, but the single-hidden-layer case is the most fundamental and widely used. You can refer to the original paper by Kurt Hornik for more details.</p>
<p>I hope this blog helped you understand the Universal Approximation Theorem better. If you have any questions or feedback, feel free to reach out to me. Thanks for reading!</p>
<pre tabindex="0"><code></code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://iamsecretlyflash.github.io/">Vaibhav Seth</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
